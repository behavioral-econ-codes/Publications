{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import artm\n",
    "\n",
    "data_dir='.\\\\data\\\\'\n",
    "data_file='Ec_Bu__be_vpw.txt'\n",
    "\n",
    "# ----  BIGARTM !!! ------\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='data/Ec_Bu__be_vpw.txt',\n",
    "                                        data_format='vowpal_wabbit',\n",
    "                                        target_folder='.\\\\collection_batches3\\\\',\n",
    "                                        class_ids=['text', 'authors', 'bigrams', 'references'] #'text',\n",
    "                                       )\n",
    "\n",
    "main_dictionary = artm.Dictionary()\n",
    "main_dictionary.gather(data_path='collection_batches3')\n",
    "main_dictionary.save(dictionary_path='collection_batches3/main_dictionary')\n",
    "main_dictionary.save_text(dictionary_path='collection_batches3/main_dictionary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# просматриваем получившийся файл в панде\n",
    "data_dir='.\\\\collection_batches3\\\\'\n",
    "data_file='main_dictionary.txt'  \n",
    "header=['token', 'class_id', 'value', 'tf', 'df']\n",
    "main_pd = pd.read_csv(data_dir+data_file, delimiter=',', names=header, encoding=\"utf8\")\n",
    "#main_pd = main_pd.sort_values(by ='df', ascending=False)\n",
    "main_pd_=main_pd[2:]\n",
    "print main_pd.shape[0]\n",
    "main_pd_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pd_['df'].fillna(0, inplace=True)\n",
    "main_pd_['df'] = main_pd_['df'].astype('float')\n",
    "\n",
    "main_pd_.sort_values(by=['df'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Число слов ', main_pd_.loc[(main_pd_.class_id==' text')].shape[0]\n",
    "print 'Число биграмм ', main_pd.loc[(main_pd.class_id==' bigrams')].shape[0]\n",
    "print 'Число ссылок', main_pd.loc[(main_pd.class_id==' references')].shape[0]\n",
    "print 'Число авторов', main_pd.loc[(main_pd.class_id==' authors')].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dictionary.filter(class_id='text', min_df=2, max_df=5000)   #\n",
    "main_dictionary.filter(class_id='bigrams', min_df=2)   #сохранить токены с df=[2, ...) !! предварительно сохранив копию, т.к. фильтр вносит изменения сразу в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dictionary.filter(class_id='authors', max_df=630)   # убираем так Nan,  Nan df=635\n",
    "main_dictionary.filter(class_id='references', max_df=12000) # убираем так Nan,  Nan df=12093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dictionary.save_text(dictionary_path='collection_batches3/main_dictionary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import artm\n",
    "\n",
    "# -- Вводим основные (domain - d) и фоновые (Background - b) темы. \n",
    "\n",
    "def SetTopicsNum(d, b):\n",
    "    background_topics = []\n",
    "    domain_topics = []\n",
    "    all_topics = []\n",
    "\n",
    "    for i in range(1, d+b+1):\n",
    "        if i <= d:\n",
    "            topic_name = \"d\" + str(i)\n",
    "            domain_topics.append(topic_name)\n",
    "        else:\n",
    "            topic_name = \"b\" + str(i)\n",
    "            background_topics.append(topic_name)\n",
    "        all_topics.append(topic_name)\n",
    "    return all_topics, domain_topics, background_topics\n",
    "# ---\n",
    "#-- Инициализуем модель\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='collection_batches3',\n",
    "                                        data_format='batches')\n",
    "main_dictionary = artm.Dictionary()\n",
    "main_dictionary.load_text(dictionary_path='collection_batches3/main_dictionary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetScores(model):\n",
    "    \n",
    "    model.scores.add(artm.PerplexityScore(name='Perplexity_Score',\n",
    "                                      dictionary=main_dictionary))\n",
    " \n",
    "    model.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore_bigrams', \n",
    "                                           class_id='bigrams'))         \n",
    "    \n",
    "    model.scores.add(artm.SparsityThetaScore(name='SparsityThetaScore')) \n",
    "    \n",
    "    model.scores.add(artm.TopTokensScore(name='Bigrams_Top10_Tokens', \n",
    "                                         num_tokens=15, \n",
    "                                         dictionary = main_dictionary,\n",
    "                                         class_id='bigrams'))\n",
    "    \n",
    "    model.scores.add(artm.TopTokensScore(name='Text_Top10_Tokens', \n",
    "                                         num_tokens=15, \n",
    "                                         dictionary = main_dictionary,\n",
    "                                         class_id='text'))\n",
    "    \n",
    "    model.scores.add(artm.TopTokensScore(name='References_Top10', \n",
    "                                         num_tokens=15, \n",
    "                                         dictionary = main_dictionary,\n",
    "                                         class_id='references'))\n",
    "                    \n",
    "    model.scores.add(artm.TopTokensScore(name='Authors_Top15', \n",
    "                                         num_tokens=15, \n",
    "                                         dictionary = main_dictionary,\n",
    "                                         class_id='authors'))\n",
    "        \n",
    "    model.scores.add(artm.TopicKernelScore(name='DomainTopicKernelScore', \n",
    "                                           probability_mass_threshold=0.25, \n",
    "                                           class_id='bigrams' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_dict - словарь OrderedDict, ключ - имя регуляризатора, значение - коэффициент\n",
    "#Оставляем только те регуляризаторы, которые нужны\n",
    "\n",
    "def SetRegularizers(model, reg_dict):\n",
    "    \n",
    "    #model.regularizers.add(artm.DecorrelatorPhiRegularizer(name=reg_dict.items()[0][0], gamma=0, tau=reg_dict[reg_dict.items()[0][0]])) #, class_ids=['text', 'bigrams', 'authors'], topic_names=domain_topics))\n",
    "    #model.regularizers.add(artm.DecorrelatorPhiRegularizer(name=reg_dict.items()[1][0], gamma=0, class_ids=['references'], tau=reg_dict[reg_dict.items()[1][0]]))\n",
    "    #model.regularizers.add(artm.SmoothSparsePhiRegularizer(name=reg_dict.items()[0][0], gamma=0, topic_names=background_topics, class_ids=['text', 'bigrams'], dictionary=main_dictionary, tau=reg_dict[reg_dict.items()[0][0]]))\n",
    "\n",
    "    model.regularizers.add(artm.SmoothSparsePhiRegularizer(name=reg_dict.items()[0][0], gamma=0, topic_names=domain_topics, tau=reg_dict[reg_dict.items()[0][0]]))\n",
    "    model.regularizers.add(artm.SmoothSparseThetaRegularizer(name=reg_dict.items()[1][0], topic_names=domain_topics, tau=reg_dict[reg_dict.items()[1][0]]))\n",
    "#     model.regularizers.add(artm.SmoothSparsePhiRegularizer(name=reg_dict.items()[4][0], gamma=0, topic_names=domain_topics, tau=reg_dict[reg_dict.items()[4][0]]))\n",
    "#     model.regularizers.add(artm.SmoothSparseThetaRegularizer(name=reg_dict.items()[5][0], topic_names=domain_topics, tau=reg_dict[reg_dict.items()[5][0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "results_dir='.\\\\final_tests4\\\\'\n",
    "\n",
    "# для тестирования моделей задаем листы - в зависимости от задачи тестирования\n",
    "top_num_list =[13]  #для тестирования числа тем\n",
    "#для тестирования значения коэффицикнтов регуляризации:\n",
    "tau_list1=[-0.6]   \n",
    "tau_list2=[-2.81]  \n",
    "\n",
    "for top_num in top_num_list:\n",
    "\n",
    "#задаем параметры моделей\n",
    "\n",
    "    class_ids={'bigrams':1.0, 'authors':1.0, 'references':1.0}  #{'text': 0.5, 'bigrams':1.0, 'authors':1.0, 'references':1.0}\n",
    "    all_topics, domain_topics, background_topics=SetTopicsNum(top_num, 0)  #SetTopicsNum(d, b)\n",
    "    reg_dict = OrderedDict([\n",
    "                            #('DecorrPhi', t1),\n",
    "                            #('SmoothPhi_back', t1)\n",
    "                          ('SparsePhi', t1),\n",
    "                          ('SparseTheta', t2),\n",
    "                                    ]\n",
    "                                    )\n",
    "    \n",
    "    model=artm.ARTM(topic_names = all_topics,\n",
    "                        dictionary=main_dictionary,\n",
    "                        class_ids=class_ids,\n",
    "                        cache_theta=True,\n",
    "                        theta_columns_naming='title',\n",
    "                        seed=2) \n",
    "    \n",
    "    #name_to_save='seed_test_10step_'+str(seed_num)\n",
    "    #name_to_save='num_test_10step_'+str(top_num)+'_topics'\n",
    "    model.initialize(dictionary=main_dictionary)\n",
    "#     model.load(name_to_save)   #10step_none_reg_15topics_2  \n",
    "#     model.class_ids={'text': 0.5, 'bigrams':1.0, 'authors':1.0, 'references':1.0}\n",
    "#     all_topics, domain_topics, background_topics=SetTopicsNum(top_num, 0)\n",
    "    \n",
    "    SetScores(model)\n",
    "\n",
    "    SetRegularizers(model, reg_dict)\n",
    "\n",
    "    model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=10)\n",
    "\n",
    "    CallingResultsProcedures(0, model)\n",
    "    #model.save(name_to_save,'p_wt')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызов функций для записи результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_figures(model, topics_directory):\n",
    "\n",
    "    # perplexity\n",
    "    x = range(model.num_phi_updates)[1:]\n",
    "    fig, ax1 = plt.subplots()\n",
    "    #plt.title(u'Метрики качества модели, 15*'+ '$\\\\tau$/|$W_t$|, $\\\\tau$='+ str(format(tau_value, \".0e\")), fontsize=14, y=1.06)\n",
    "    \n",
    "    ax1.plot(x, model.score_tracker['Perplexity_Score'].value[1:], 'g-', linewidth=1, label=u\"Перплексия\")\n",
    "    ax1.set_xlabel(u'Номер итерации')\n",
    "    ax1.set_ylabel(u'Перплексия', color='g')\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "   # ax2.plot(x, model.score_tracker['SparsityPhiScore_text'].value[1:], 'r*', linewidth=1, label=u'Разреженность '+'$\\\\Phi$'+'-text')\n",
    "    ax2.plot(x, model.score_tracker['SparsityPhiScore_bigrams'].value[1:], 'r:', linewidth=1, label=u'Разреженность '+'$\\\\Phi$'+'-bigrams')\n",
    "    ax2.plot(x, model.score_tracker['SparsityThetaScore'].value[1:], 'r-.', linewidth=1, label=u'Разреженность '+'$\\\\Theta$')\n",
    "    ax2.set_ylabel(u'Доля', color='r')\n",
    "    ax2.legend(bbox_to_anchor=(1.10, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.3)\n",
    "    \n",
    "    \n",
    "    ax1.text(0.14, -0.16, u'Перплексия: ' + str(round(model.score_tracker['Perplexity_Score'].last_value, 3))+\n",
    "            # u'\\nРазреженность ' +'$\\\\Phi$'+'-text: ' + str(round(model.score_tracker['SparsityPhiScore_text'].last_value, 3))+\n",
    "             u',  Разреженность ' +'$\\\\Phi$'+'-bigrams: ' +str(round(model.score_tracker['SparsityPhiScore_bigrams'].last_value, 3))+\n",
    "             u'\\nРазреженность ' +'$\\\\Theta$: '+str(round(model.score_tracker['SparsityThetaScore'].last_value,3)), \n",
    "         transform=plt.gcf().transFigure, bbox=props)\n",
    "\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.savefig(topics_directory+'scores_sparsity.png', dpi=150, bbox_inches = 'tight')\n",
    "    #plt.show()\n",
    "    \n",
    "    # kernels\n",
    "    x = range(model.num_phi_updates)[1:]\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(x, model.score_tracker['DomainTopicKernelScore'].average_size[1:], 'g-', linewidth=1, label=u\"Размер ядра\")\n",
    "    ax1.set_xlabel(u'Номер итерации')\n",
    "    ax1.set_ylabel(u'Размер ядра', color='g')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, model.score_tracker['DomainTopicKernelScore'].average_contrast[1:], 'r*', linewidth=1, label=u\"Контраст\")\n",
    "    ax2.plot(x, model.score_tracker['DomainTopicKernelScore'].average_purity[1:], 'r--', linewidth=1, label=u\"Чистота\")\n",
    "    ax2.set_ylabel(u'Доля', color='r')\n",
    "    ax2.legend(bbox_to_anchor=(1.10, 1), loc=2, borderaxespad=0.)\n",
    "    kernel=model.score_tracker['DomainTopicKernelScore'].last_average_size\n",
    "    last_average_contrast=model.score_tracker['DomainTopicKernelScore'].last_average_contrast\n",
    "    last_average_purity=model.score_tracker['DomainTopicKernelScore'].last_average_purity\n",
    "    ax1.text(0.14, -0.07, u'Размер ядра: ' + str(round(kernel, 3))+\n",
    "             u',  Контраст: ' + str(round(last_average_contrast, 3))+\n",
    "             u',  Чистота: ' + str(round(last_average_purity, 3)),\n",
    "         transform=plt.gcf().transFigure, bbox=props)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.savefig(topics_directory+'scores_kernel.png', dpi=150, bbox_inches = 'tight')\n",
    "    \n",
    "    return kernel, last_average_contrast, last_average_purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CallingResultsProcedures(step, model):\n",
    "    TopicTrackerTable_cur, rows_num=ReadTableResults()\n",
    "    print rows_num\n",
    "    topics_directory=CreateFilesWithTopics(rows_num)\n",
    "    kernel, last_average_contrast, last_average_purity = plot_figures(model, topics_directory)\n",
    "    WritingModelsResultsToFile(step, TopicTrackerTable_cur, rows_num, topics_directory, kernel, last_average_contrast, last_average_purity)\n",
    "    \n",
    "    # Записываем значения перплексии\n",
    "    with open(results_dir+\"perplexities.txt\", 'a') as f:\n",
    "        f.write(\"\\n\")\n",
    "        for perp in model.score_tracker['Perplexity_Score'].value:\n",
    "            f.write(\"%s, \" % perp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запись итогов работы модели и когерентрости в единый файл результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шапка таблицы была сформирована заранее\n",
    "#TopicTrackerTable = pd.DataFrame(columns=['topics num', 'class_ids', 'steps num', 'strategy', 'perplexity', 'Phi-sparcity', 'Theta-sparcity', 'Kernel size', 'Contrast', 'Purity', 'Coherence'])\n",
    "# Считываем текущую таблицу на предмет определения числа строк в ней, чтобы последующую запись записать ниже\n",
    "\n",
    "model_results_file='models_testing.csv'\n",
    "\n",
    "def ReadTableResults():    \n",
    "    TopicTrackerTable_cur = pd.read_csv(results_dir+model_results_file)\n",
    "    rows_num= TopicTrackerTable_cur.shape[0]\n",
    "    return TopicTrackerTable_cur, rows_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем новую директорию формата folder_name + rows_num (число строк)\n",
    "# сохраняем все топ-токены всех тем в отдельные файлы новой директории\n",
    "\n",
    "import os\n",
    "\n",
    "def CreateFilesWithTopics(rows_num):\n",
    "\n",
    "    folder_name=\"topic_tokens_\"+str(rows_num+1)\n",
    "    topics_directory=results_dir + folder_name + \"\\\\\"\n",
    "    if not os.path.exists(topics_directory):\n",
    "        os.makedirs(topics_directory)\n",
    "#     else:\n",
    "#         os.remove(topics_directory)\n",
    "\n",
    "    top_tokens = model.score_tracker[\"Bigrams_Top10_Tokens\"]\n",
    "    \n",
    "    for topic_name in all_topics:\n",
    "        data_file_name=topic_name+'_tokens.txt'\n",
    "        with open(topics_directory+data_file_name, 'w') as f:\n",
    "            for (item, weight) in zip(top_tokens.last_tokens[topic_name], top_tokens.last_weights[topic_name]):\n",
    "                f.write(item.decode('utf8') +', '+str(round(weight,5))+'\\n')\n",
    "    return topics_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===  Подсчет когерентности  =======\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_dir='.\\\\data\\\\'\n",
    "\n",
    "file_name='Ec_Bu__be_p1_p2_filtered_preprocessed2.csv'\n",
    "#header=['_id', 'authors', 'fieldsOfStudy', 'id', 'inCitations', 'journalName', 'outCitations', 'paperAbstract', 'title','year']\n",
    "\n",
    "table_prep = pd.read_csv(data_dir+file_name, delimiter=',', quotechar='\"', error_bad_lines=False, engine='python')\n",
    "table_prep['bigrams'] = table_prep['bigrams'].apply(lambda x: frozenset([word.strip() for word in x.split('|||')]))\n",
    "\n",
    "\n",
    "def CoherenceCalculating(topics_directory):\n",
    "    \n",
    "    # открываем и считываем все слова всех тем - имя директории (topics_directory) зависит от числа строк в таблице результатов\n",
    "    topic_files = [i for i in os.listdir(topics_directory) if (i !='coherence.txt')&(i.endswith(\".txt\")&(i.startswith(\"d\")))]\n",
    "    print len(topic_files)\n",
    "    # словарь, ключи которого - названия топиков (d8_tokens, b10_tokens и т.п.), \n",
    "    # элементы - кортеж: (словарь слов->частоты, словарь пар->частоты(+PMI), связность)   \n",
    "    topics_dict = {}\n",
    "#     topics_coherence = {}  # формируем пустой словарь для связности\n",
    "\n",
    "    # Формируем словарь из тем с элементами:\n",
    "    for topic_file in topic_files:\n",
    "#         print topic_file    \n",
    "        \n",
    "        # считываем  топ-слова темы\n",
    "        topic_words = []    \n",
    "        with open(topics_directory+topic_file, 'r') as f:\n",
    "            #topic_words = [line.strip('\\n') for line in f]\n",
    "            topic_words = [line.split(',')[0] for line in f]   # в файле и слово, и значение, берем только слово [0]\n",
    "    \n",
    "        words = dict.fromkeys(topic_words,0)\n",
    "        \n",
    "        # составляем всевозможные пары комбинаций (сразу конвертируем в наборы)\n",
    "        pairs = dict.fromkeys([frozenset(p) for p in itertools.combinations(topic_words, 2)],0)\n",
    "        \n",
    "        coherence = 0.0\n",
    "        \n",
    "        # имя топика = имя файла (без .txt)\n",
    "        topics_dict[topic_file[:-4]] = (words, pairs, coherence) # [i for i in itertools.combinations(topic_words, 2)])  \n",
    "\n",
    "    rows = table_prep['bigrams']#[-10000:]\n",
    "\n",
    "    # iterate over rows and then over topics and calculate counts of words & pairs\n",
    "    for row in rows:\n",
    "        for topic in topics_dict:\n",
    "            # unpack tuple\n",
    "            words, pairs, coherence = topics_dict[topic]\n",
    "            \n",
    "            # increment word\n",
    "            for word in words:\n",
    "                words[word] += int (word in row)\n",
    "            \n",
    "            # increment pair\n",
    "            for pair in pairs:\n",
    "                pairs[pair] += int(pair.issubset(row))\n",
    "            \n",
    "            # re-pack   \n",
    "            topics_dict[topic] = (words, pairs, coherence)\n",
    "        \n",
    "    D = len(rows)\n",
    "    \n",
    "    # calculate PMI coherence by topic\n",
    "    for topic in topics_dict:\n",
    "        # unpack tuple\n",
    "        words, pairs, coherence = topics_dict[topic]\n",
    "           \n",
    "        # calc PMI and cusum them for all pairs\n",
    "        for pair in pairs:\n",
    "            PMI = calc_PMI(pair, pairs, words, D)\n",
    "            coherence += PMI\n",
    "            # now save tuple of count and PMI for pair\n",
    "            pairs[pair] = pairs[pair], PMI\n",
    "         \n",
    "        # normalize PMI by pairs count\n",
    "        coherence = coherence/float(len(pairs)) # = k(k-1)/2\n",
    "\n",
    "        # re-pack\n",
    "        topics_dict[topic] = (words, pairs, coherence)\n",
    "\n",
    "    # save results and cumulate coherence\n",
    "    coherence = 0.0\n",
    "    \n",
    "    with open(topics_directory+\"coherence.txt\", 'w') as f:\n",
    "        for topic in topics_dict:\n",
    "            coherence += topics_dict[topic][2]\n",
    "            f.write(topic + \" \" +str(topics_dict[topic][2])+ \"\\n\")\n",
    "        \n",
    "        coherence = coherence/float(len(topics_dict))\n",
    "        f.write(\"Итоговая средняя когерентность = \" + str(coherence))  \n",
    "    \n",
    "    return coherence, topics_dict\n",
    "       \n",
    "def calc_PMI(pair, pairs, words, D, set_zero_count_by=1e-6):\n",
    "    \"\"\"\n",
    "    Pointwise Mutual Information\n",
    "    \n",
    "    where:\n",
    "    D - vol of corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    word = tuple(pair)\n",
    "    \n",
    "    # if smth is rare\n",
    "    \n",
    "    N12 = pairs[pair]\n",
    "    if pairs[pair] == 0:\n",
    "        N12 = set_zero_count_by*set_zero_count_by\n",
    "\n",
    "    N1 = words[word[0]]\n",
    "    if N1 == 0:\n",
    "        N1 = set_zero_count_by\n",
    "    \n",
    "    N2 = words[word[1]]\n",
    "    if words[word[1]] == 0:\n",
    "        N2 = set_zero_count_by\n",
    "        \n",
    "    return np.log(float(D*N12)/float(N1*N2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WritingModelsResultsToFile(steps, TopicTrackerTable_cur,rows_num, topics_directory, kernel, last_average_contrast, last_average_purity):\n",
    "# ---- Записываем в линию значения коэффициентов регуляризации. \n",
    "#Если регуляризатор не включен - будет ошибка, поэтому помещаем в try\n",
    "    DecorrPhi=\"\"\n",
    "    SmoothPhi=\"\"\n",
    "    SparsePhi=\"\"\n",
    "    SparseTheta=\"\"\n",
    "    SmoothPhi_back=\"\"\n",
    "    SmoothTheta_back=\"\"\n",
    "    SmoothPhi_i=\"\"\n",
    "    #DecorrPhi_ref=\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        #DecorrPhi = str(\"{:.2e}\".format(model.regularizers['DecorrPhi'].tau)) \n",
    "        #DecorrPhi_ref = str(\"{:.2e}\".format(model.regularizers['DecorrPhi_ref'].tau)) \n",
    "        \n",
    "        \n",
    "        #SmoothPhi_back = str(\"{:.2e}\".format(model.regularizers['SmoothPhi_back'].tau))\n",
    "        #SmoothPhi_i = str(\"{:.2e}\".format(tau))\n",
    "        \n",
    "        #SmoothTheta_back = str(\"{:.2e}\".format(model.regularizers['SmoothTheta_back'].tau))\n",
    "        #SmoothPhi_d = str(\"{:.2e}\".format(model.regularizers['SmoothPhi_d'].tau))\n",
    "#         SmoothPhi_a = str(\"{:.2e}\".format(model.regularizers['SmoothPhi_a'].tau))\n",
    "        SparsePhi = str(\"{:.2e}\".format(model.regularizers['SparsePhi'].tau))\n",
    "        SparseTheta = str(\"{:.2e}\".format(model.regularizers['SparseTheta'].tau))\n",
    "        \n",
    "    \n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "    strategy_line= \"; \".join([SparsePhi, \n",
    "                              SmoothPhi_back,\n",
    "                              #SmoothPhi_i,\n",
    "                              #DecorrPhi,\n",
    "#                               DecorrPhi_ref,\n",
    "#                               ImproveCoherence\n",
    "                              \n",
    "                               #,\n",
    "                              #SmoothPhi_b, SmoothPhi_a, \n",
    "                              SparseTheta\n",
    "                              \n",
    "                             ])\n",
    "# ----\n",
    "\n",
    "    perpl=str(\"{:.2e}\".format(model.score_tracker['Perplexity_Score'].last_value))\n",
    "    coherence, topics_dict = CoherenceCalculating(topics_directory)\n",
    "    \n",
    "    TopicTrackerTable_cur.loc[rows_num+1]=[rows_num+1, \n",
    "                                         len(all_topics), \n",
    "                              model.class_ids, # class_ids\n",
    "                              len(model.score_tracker['Perplexity_Score'].value)+steps,  # число шагов\n",
    "                              str(model.regularizers)+ \": \" + strategy_line,                      \n",
    "                              perpl, \n",
    "                              round(model.score_tracker['SparsityPhiScore_bigrams'].last_value, 4),\n",
    "                              round(model.score_tracker['SparsityThetaScore'].last_value, 4),\n",
    "                              round(kernel,4),\n",
    "                              round(last_average_contrast,4),\n",
    "                              round(last_average_purity,4),\n",
    "                              round(coherence,4)\n",
    "                             ]\n",
    "    TopicTrackerTable_cur.to_csv(results_dir+'models_testing.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод результатов модели!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  --  Печать топ-токенов тем  ---\n",
    "\n",
    "top_tokens = model.score_tracker[\"Bigrams_Top10_Tokens\"]  #References_Top10  Bigrams_Top10_Tokens Text_Top10_Tokens #Authors_Top15\n",
    "\n",
    "for topic_name in model.topic_names:\n",
    "    token_line_list=[]\n",
    "    print (topic_name)\n",
    "    for (token, weight) in zip(top_tokens.last_tokens[topic_name],\n",
    "                               top_tokens.last_weights[topic_name]):    \n",
    "         token_line_list.append(token)\n",
    "        #print token  #, '-', round(weight,3)\n",
    "    print ', '.join(token_line_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Сохраняем авторов и их id. Сохраняем все ссылки в список. Чтобы потом расшифровать\n",
    "folder_name='authors\\\\'\n",
    "\n",
    "def GetIdbyAuthorName(name):\n",
    "    \n",
    "    if name<>u'nan':\n",
    "        \n",
    "        tab=table_raw[table_raw['authors_name'].str.contains(name.encode('utf8'))][['authors_name', 'authors_ids']][:1]\n",
    "       \n",
    "        authors_list=str(tab.iloc[0,0]).split(' ')\n",
    "       \n",
    "        author_id = None\n",
    "        for index, i in enumerate(authors_list):\n",
    "            if i==name.encode('utf8'):\n",
    "                author_id=str(tab.iloc[0,1]).split(' ')[index]\n",
    "                break\n",
    "        if author_id == None:\n",
    "            author_id=\"\"\n",
    "            #print authors_list\n",
    "        return author_id\n",
    "\n",
    "\n",
    "top_tokens = model.score_tracker[\"Authors_Top15\"]\n",
    "\n",
    "table_raw['authors_name'] = table_raw['authors_name'].fillna(\"\")\n",
    "\n",
    "\n",
    "for topic_name in model.topic_names:\n",
    "    with open(results_dir+folder_name+topic_name+'.txt', 'w') as f:\n",
    "        for (token, weight) in zip(top_tokens.last_tokens[topic_name], top_tokens.last_weights[topic_name]):\n",
    "            if token != u'nan':\n",
    "                author_id=GetIdbyAuthorName(token)  #Andreas_Löschel\n",
    "#                 if author_id is None:\n",
    "#                     #print len(token), token\n",
    "#                     author_id=\" \"\n",
    "#                     break\n",
    "                f.write(token.encode('utf-8')+', '+author_id.encode('utf8'))    # +', '+str(round(weight,5))+'\\n')\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Сохраняем все ссылки в список. Чтобы потом расшифровать\n",
    "top_tokens = model.score_tracker[\"Bigrams_Top10_Tokens\"]  #Bigrams_Top10_Tokens   References_Top10\n",
    "folder_name='bigrams\\\\'\n",
    "#folder_name='references\\\\'\n",
    "\n",
    "\n",
    "for topic_name in model.topic_names:\n",
    "    with open(results_dir+folder_name+topic_name+'.txt', 'w') as f:\n",
    "        #f.write(topic_name +'\\n')\n",
    "        for (token, weight) in zip(top_tokens.last_tokens[topic_name], top_tokens.last_weights[topic_name]): \n",
    "            if token != u'nan':\n",
    "                f.write(token.encode('utf-8') +', '+str(round(weight,5))+'\\n')\n",
    "                #f.write(token.encode('utf-8') +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем из модели Тета\n",
    "theta = model.get_theta()\n",
    "theta_tr = theta.transpose(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открываем исходный файл:\n",
    "import pandas as pd\n",
    "data_dir='.\\\\data\\\\'\n",
    "file_name='Ec_Bu__be_p1_p2_filtered_preprocessed2.csv'\n",
    "#header=['_id', 'authors', 'fieldsOfStudy', 'id', 'inCitations', 'journalName', 'outCitations', 'paperAbstract', 'title','year']\n",
    "\n",
    "table_raw = pd.read_csv(data_dir+file_name, delimiter=',', quotechar='\"', error_bad_lines=False, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_raw.set_index('_id', inplace=True)\n",
    "table_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Соединяем Тету и table_raw\n",
    "table_theta=theta_tr.join(table_raw)\n",
    "data_file='table_theta.csv'\n",
    "table_theta.to_csv(results_dir+data_file, sep=';', encoding='utf8', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим по теме публикации:\n",
    "import numpy as np \n",
    "    \n",
    "def GetTopicDocs(d, alpha):\n",
    "    def isnan(value):\n",
    "        try:\n",
    "            import math\n",
    "            return math.isnan(float(value))\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    topic_name='d'+str(d)\n",
    "    print '\\n'\n",
    "    print topic_name\n",
    "    table_d=table_theta.sort_values(by=[topic_name], ascending=False)\n",
    "    table_d_threshhold=table_d.loc[table_d[topic_name]>alpha]\n",
    "    \n",
    "#----  print some results:  ------\n",
    "    for index, row in table_d_threshhold.iterrows():\n",
    "        authors_name=row['authors_name']\n",
    "       \n",
    "        if isnan(authors_name)==False:\n",
    "            authors_name=', '.join(authors_name.split())\n",
    "            ahref_scr='https://www.semanticscholar.org/paper/'+row['id']\n",
    "            #print row['id']\n",
    "            print authors_name + \" (\" + str(int(row['year'])) +\"). \" + row['title']\n",
    "# ---------\n",
    "    \n",
    "    results_dir_theta=results_dir+'theta_docs\\\\'\n",
    "    table_d_threshhold.to_csv(results_dir_theta+'theta_'+topic_name+'.csv', sep=';', encoding='utf8', index=True, header=True)\n",
    "    return\n",
    "\n",
    "GetTopicDocs(11, 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
